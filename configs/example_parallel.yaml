name: example-parallel
backend: vllm
sampling_mode: parallel
model: meta-llama/Llama-3.1-8B-Instruct
prompts_path: ../data/example_prompts.txt
output_dir: ../outputs
output_filename: samples_parallel.jsonl
prompt_template: "Answer concisely: {prompt}"
system_prompt: You are a concise assistant.
max_tokens: 64
temperature: 0.7
top_p: 0.95
top_k: 5
num_generations: 2   # fan-out per prompt in parallel sampler
seed: 42
vllm_base_url: http://localhost:8000/v1
vllm_api_key: dummy-key
request_timeout: 3600
max_concurrent: 1
vllm_host: 127.0.0.1
vllm_port: 8000
vllm_server_args:
  - "--max-num-batched-tokens"
  - "8192"
