# vLLM Runtime Configs

This folder defines runtime-specific configs for launching a vLLM server command.

The command is generated by:

`configs/vllm_config/vllm_server_runner.py`

It takes:
- one model config in the existing format at `configs/model_configs/*.yaml`
- one runtime config in this folder (`apptainer` or `local_python`)

## Quick Usage

Apptainer/Docker-like launch command:

```bash
.venv/bin/python configs/vllm_config/vllm_server_runner.py \
  --model-config configs/model_configs/gpt-oss-20b.yaml \
  --vllm-config configs/vllm_config/apptainer.yaml
```

Local Python launch command (might not support all libraries depending on the cluster configuration):

```bash
.venv/bin/python configs/vllm_config/vllm_server_runner.py \
  --model-config configs/model_configs/gpt-oss-20b.yaml \
  --vllm-config configs/vllm_config/local_python.yaml
```

The script prints a single bash command.

Optional runtime overrides:

```bash
--host 127.0.0.1 --port 8001
```

## Model Config Contract (existing)

The runner reads:
- `model` (required)
- `vllm_server_args` (optional list of CLI args)

Example:

```yaml
model: openai/gpt-oss-20b
vllm_server_args:
  - "--max-model-len"
  - "16384"
```

## Runtime Config Schema

### 1) `runtime: apptainer`

Required:
- `image_sif`: path to SIF image
- `cache_base`: base cache directory on host

Optional:
- `host` (default `127.0.0.1`)
- `port` (default `8000`)
- `entrypoint` (`serve` or `api_server`, default `serve`)
- `apptainer_bin` (default `apptainer`)
- `vllm_bin` (default `vllm`)
- `python_bin` (default `python`)
- `use_gpu` (default `true`)
- `cleanenv` (default `true`)
- `container_cache_root` (default `<cache_base>/.cache`)
- `extra_server_args` (list, default `[]`)
- `hf_token` (optional static token value)

Behavior:
- Creates cache folders under `cache_base`
- Binds host cache paths into container
- Sets HF/vLLM/torch/flashinfer env vars
- Appends model `vllm_server_args`, then `extra_server_args`

### 2) `runtime: local_python`

Optional:
- `host` (default `127.0.0.1`)
- `port` (default `8000`)
- `entrypoint` (`api_server` or `serve`, default `api_server`)
- `python_bin` (default current interpreter path)
- `vllm_bin` (default `vllm`)
- `use_default_cache_env` (default `true`)
- `cache_base` (default `$SLURM_TMPDIR/parseq_vllm_cache` or `/tmp/$USER/parseq_vllm_cache`)
- `env` (dict of `KEY: value`, merged after defaults)
- `extra_server_args` (list, default `[]`)
- `hf_token` (optional static token value; exported as `HF_TOKEN` if not already set)

Behavior:
- Builds a direct local command
- Creates writable cache dirs and exports cache env vars by default:
  `XDG_CACHE_HOME`, `HF_HOME`, `HUGGINGFACE_HUB_CACHE`, `VLLM_CACHE_DIR`,
  `TORCH_EXTENSIONS_DIR`, `TRITON_CACHE_DIR`, `FLASHINFER_*`
- Merges explicit `env` overrides after defaults
- Appends model `vllm_server_args`, then `extra_server_args`
