name: example-enumeration
backend: vllm
sampling_mode: enumeration
model: meta-llama/Llama-3.1-8B-Instruct
prompts_path: ../data/example_prompts.txt
output_dir: ../outputs
output_filename: samples.jsonl
prompt_template: "Answer concisely: {prompt}"
system_prompt: You are a concise assistant.
first_turn_instruction: "Solve the task."
response_instruction: "Generate diverse candidate answers."
max_tokens: 128
temperature: 0.8
top_p: 0.95
top_k: 5
num_generations: 3
n_parallel: 1
history_k: 1
constrained_gen:
  json_schema_strict: true
vllm_base_url: http://localhost:8000/v1
vllm_api_key: dummy-key
request_timeout: 3600
max_concurrent: 16
vllm_host: 127.0.0.1
vllm_port: 8000
