model: Qwen/Qwen3-32B
ngpus: 4
vllm_runtime_config: configs/vllm_config/local_python.yaml
vllm_server_args:
  - "--tensor-parallel-size"
  - "4"
  - "--dtype"
  - "bfloat16"
  - "--max-model-len"
  - "32768"
  - "--gpu-memory-utilization"
  - "0.92"
  - "--max-num-batched-tokens"
  - "16392"
  - "--max-num-seqs"
  - "8"
  - "--disable-uvicorn-access-log"
  - "--generation-config"
  - "vllm"
scenario_override:
  temperature: 0.7
  top_k: 20
  top_p: 0.8
  max_tokens: 8192
  vllm_extra_body:
    chat_template_kwargs:
      enable_thinking: false
